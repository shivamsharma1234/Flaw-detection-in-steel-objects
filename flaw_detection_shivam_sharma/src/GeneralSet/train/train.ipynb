{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended, /src/Mask_RCNN to PATH\n",
      "/home/shivam/workspace/flaw_detection/src/GeneralSet/train/../../\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import os \n",
    "import sys\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(os.getcwd())#os.path.dirname(os.path.realpath(__file__))#S\n",
    "import sys\n",
    "sys.path.append(os.path.join(ROOT_DIR,'../../'))\n",
    "print(\"Appended, /src/Mask_RCNN\" + \" to PATH\") #src/ folder contains common files required by all experiments\n",
    "\n",
    "sys.path.append(os.path.join(ROOT_DIR,'../..//Mask_RCNN'))\n",
    "print(os.path.join(ROOT_DIR,'../../'))\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "from imgaug import augmenters as iaa\n",
    "import config\n",
    "#import paths\n",
    "# Import Mask RCNN\n",
    "from Mask_RCNN.mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "import datasets\n",
    "import imgaug\n",
    "import yaml\n",
    "import argparse\n",
    "import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = os.path.join(ROOT_DIR,'../../../logs')\n",
    "dataset = os.path.join(ROOT_DIR,'../../../data/flaw_detection')\n",
    "weights = \"coco\"\n",
    "reinitialize= False\n",
    "set_types='types'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install numpy\n",
    "#pip3 install scikit-image\n",
    "#pip3 install imgaug\n",
    "# Root directory of the project\n",
    "# ROOT_DIR =  os.getcwd() + \"/src/Mask_RCNN\"\n",
    "# Path to trained weights file\n",
    "# COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "#%%\n",
    "# Directory to save logs and model checkpoints, if not provided\n",
    "# through the command line argument --logs\n",
    "# DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "set_config = None\n",
    "args = None\n",
    "types = None\n",
    "\n",
    "def yml_read(config_yml):\n",
    "\n",
    "    global set_config\n",
    "    with open(config_yml) as f:\n",
    "        set_config = yaml.load(f, Loader=yaml.FullLoader)['Flaws']\n",
    "    types = set_config['types']\n",
    "        \n",
    "# def args_parse():\n",
    "\n",
    "#      # Parse command line arguments\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#     description='Train Mask R-CNN. Usage python train.py general --weights=modelfile --dataset ')\n",
    "    \n",
    "#     parser.add_argument(\"set_type\",\n",
    "#                         metavar=\"<set_type>\",\n",
    "#                         help=\"'general' or 'orthopaedic'\")\n",
    "#     parser.add_argument('--dataset', required=False,\n",
    "#                         metavar=\"/path/to/custom/dataset/\",\n",
    "#                         help='Directory of the custom dataset')\n",
    "#     parser.add_argument('--weights', required=True,\n",
    "#                         metavar=\"/path/to/weights.h5\",\n",
    "#                         help=\"Path to weights .h5 file or 'coco'\")\n",
    "#     parser.add_argument('--speciality', required=True,\n",
    "#                         metavar=\"GeneralSet\",\n",
    "#                         default=\"GeneralSet\",\n",
    "#                         help=\"enter speciality\")\n",
    "#     parser.add_argument('--config_yml', required=True,\n",
    "#                         metavar=\"/path/to/weights.h5\",\n",
    "#                         help=\"Path to config yml file\")\n",
    "#     parser.add_argument(\"--reinitialize\", required=False, action='store_true')\n",
    "#     parser.add_argument('--logs', required=False,\n",
    "#                         default=os.path.join(ROOT_DIR,\"../../../logs\"),\n",
    "#                         metavar=\"/path/to/logs/\",\n",
    "#                         help='Logs and checkpoints directory (default=logs/)')\n",
    "#     parser.add_argument('--prev_augment', default=False, \n",
    "#                     action='store_true')\n",
    "#     global args\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "def assign(config):\n",
    "   \n",
    "    # Configurations\n",
    "    # if args.set_type == \"flaw\":\n",
    "    #     config = config.FlawSetConfig(Config)\n",
    "   \n",
    "    \n",
    "    #if args.set_type != \"ring_forceps\":\n",
    "    for k, v in set_config['params']['types'].items():\n",
    "        setattr(config, k, v)\n",
    "        print(k,v)\n",
    "    # print(config)\n",
    "    config.display()\n",
    "    return config\n",
    "\n",
    "def augment():\n",
    "    prev_augment = False\n",
    "    if not prev_augment:\n",
    "        # Augmentating images based on Matterport's recommendations: See: https://github.com/matterport/Mask_RCNN/blob/master/samples/nucleus/nucleus.py\n",
    "        print(\"Augmenting Dataset\")\n",
    "        augmentation = iaa.Sequential([\n",
    "            iaa.OneOf([\n",
    "                        iaa.Fliplr(0.9),\n",
    "                        iaa.Flipud(0.9)\n",
    "                      ]),\n",
    "            iaa.OneOf([ \n",
    "                        iaa.Affine(rotate=(-90, 90)),\n",
    "                        iaa.Affine(rotate=(-30, 30)),\n",
    "                        iaa.Affine(rotate=(-30, 30)),\n",
    "                        iaa.Affine(rotate=(-270, 270))\n",
    "                      ]),\n",
    "            iaa.Affine(scale=(0.4, 0.6)),\n",
    "        ])\n",
    "    return augmentation\n",
    "\n",
    "def create_model(config_t):\n",
    "    # Create model\n",
    "    model = modellib.MaskRCNN(mode=\"training\", config=config_t,\n",
    "                                  model_dir=logs)\n",
    "    \n",
    "    # Select weights file to load\n",
    "    if weights.lower() == \"coco\":\n",
    "        weights_path = os.path.join(ROOT_DIR,\"../../../file.h5\")\n",
    "        # Download weights file\n",
    "        if not os.path.exists(weights_path):\n",
    "            utils.download_trained_weights(weights_path)\n",
    "    elif weights.lower() == \"last\":\n",
    "        # Find last trained weights\n",
    "        weights_path = model.find_last()[1]\n",
    "    elif weights.lower() == \"imagenet\":\n",
    "        # Start from ImageNet trained weights\n",
    "        weights_path = model.get_imagenet_weights()\n",
    "    else:\n",
    "        weights_path = weights\n",
    "\n",
    "\n",
    "    \n",
    "    # Load weights\n",
    "    print(\"Loading weights \", weights_path)\n",
    "    if weights.lower() == \"coco\" :\n",
    "        # Exclude the last layers because they require a matching\n",
    "        # number of classes\n",
    "        model.load_weights(weights_path, by_name=True, exclude=[\n",
    "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "    else:\n",
    "        if reinitialize:\n",
    "            model.load_weights(weights_path, by_name=True, exclude=[\n",
    "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
    "            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "        else:\n",
    "            model.load_weights(weights_path, by_name=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dataset_load():\n",
    "    # Dataset.\n",
    "    if set_type == \"types\":\n",
    "        dataset_train = datasets.FlawDataSet()\n",
    "        dataset_val = datasets.FlawDataSet()\n",
    "    \n",
    "    \n",
    "\n",
    "    dataset_train.load_custom(args.dataset, \"train\")\n",
    "    dataset_train.prepare()\n",
    "    dataset_val.load_custom(args.dataset, \"val\")\n",
    "    dataset_val.prepare()\n",
    "\n",
    "    return dataset_train,dataset_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME flaw\n",
      "IMAGES_PER_GPU 1\n",
      "NUM_CLASSES 2\n",
      "IMAGE_MIN_DIM 480\n",
      "IMAGE_MAX_DIM 640\n",
      "DETECTION_MIN_CONFIDENCE 0.5\n",
      "RPN_NMS_THRESHOLD 0.9\n",
      "VALIDATION_STEPS 100\n",
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.5\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  640\n",
      "IMAGE_META_SIZE                13\n",
      "IMAGE_MIN_DIM                  480\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.0001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           flaw\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.9\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               100\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "config 2\n",
      "Augmenting Dataset\n",
      "Loading weights  /home/shivam/workspace/flaw_detection/src/GeneralSet/train/../../../file.h5\n",
      "Loading weights from /home/shivam/workspace/flaw_detection/src/GeneralSet/train/../../../file.h5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'set_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-17d484d5ae13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# *** This training schedule is an example. Update to your needs ***\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-b55d347eb286>\u001b[0m in \u001b[0;36mdataset_load\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdataset_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mset_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"types\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlawDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlawDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_type' is not defined"
     ]
    }
   ],
   "source": [
    "config_yml = os.path.join(ROOT_DIR,'../../speciality.yaml')\n",
    "yml_read(config_yml)\n",
    "# Validate arguments\n",
    "#assert args.dataset, \"Argument --dataset is required for training\"\n",
    "\n",
    "# print(\"Set Type: \", args.set_type)\n",
    "# print(\"Weights: \", args.weights)\n",
    "# print(\"Dataset: \", args.dataset)\n",
    "# print(\"Logs: \", args.logs)\n",
    "config = Config()\n",
    "config_t = assign(config)\n",
    "\n",
    "print('config',config_t.NUM_CLASSES)\n",
    "augmentation = augment()\n",
    "\n",
    "model = create_model(config_t)\n",
    "\n",
    "dataset_train,dataset_val = dataset_load()\n",
    "\n",
    "# *** This training schedule is an example. Update to your needs ***\n",
    "# Since we're using a very small dataset, and starting from\n",
    "# COCO trained weights, we don't need to train too long. Also,\n",
    "# no need to train all layers, just the heads should do it.\n",
    "# Training Schedule for Mask RCNN\n",
    "print(\"Train network heads\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "        learning_rate=config_t.LEARNING_RATE, # made 10 times\n",
    "        epochs=40,\n",
    "        augmentation=augmentation,\n",
    "        layers='heads')\n",
    "\n",
    "\n",
    "#Finetune layers from ResNet stage 4 and up\n",
    "print(\"Fine tune Resnet stage 4 and up\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "        learning_rate=config_t.LEARNING_RATE, # made 10 times\n",
    "        epochs=120,\n",
    "        #augmentation=augmentation,\n",
    "        layers='4+')\n",
    "\n",
    "print(\"Train all layers\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "        learning_rate=config_t.LEARNING_RATE/10,  \n",
    "        epochs=300,\n",
    "        augmentation=augmentation,\n",
    "        layers='all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
